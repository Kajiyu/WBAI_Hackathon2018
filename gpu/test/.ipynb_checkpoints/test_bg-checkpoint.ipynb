{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import brica\n",
    "import gym\n",
    "\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def initialize_parameters(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 100000\n",
    "memory_size = 64\n",
    "rollout_steps = 20\n",
    "save_steps = 100\n",
    "value_coeff = 0.5\n",
    "entropy_coeff = 0.01\n",
    "grad_norm_limit = 40\n",
    "gamma = 0.99\n",
    "lambd = 1.00\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    '''\n",
    "    Actor-Critic Model\n",
    "    '''\n",
    "    def __init__(self, action_num=2, memory_size=64, d_limit=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.action_num = action_num\n",
    "\n",
    "        # Define image embedding\n",
    "        self.image_embedding_size = 2\n",
    "        \n",
    "        # Define memory\n",
    "        self.memory_rnn = nn.GRUCell(self.image_embedding_size, self.memory_size)\n",
    "\n",
    "       \n",
    "        # Resize image embedding\n",
    "        self.embedding_size = self.memory_size\n",
    "\n",
    "        # Define actor's model\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_num),\n",
    "        )\n",
    "        self.actor_mu = nn.Sequential(\n",
    "            nn.Linear(self.action_num, self.action_num),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.actor_sigma = nn.Sequential(\n",
    "            nn.Linear(self.action_num, self.action_num),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, obs, memory):\n",
    "        hidden = self.memory_rnn(obs, memory)\n",
    "        embedding = hidden\n",
    "        memory = hidden\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        mu = self.actor_mu(x)\n",
    "        sigma = self.actor_sigma(x)\n",
    "        dist_params = [mu, sigma]\n",
    "\n",
    "        x = self.critic(embedding)\n",
    "        value = x.squeeze(1)\n",
    "\n",
    "        return dist_params, value, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BG(object):\n",
    "    def __init__(self, training=True, init_weight_path=None, use_cuda=False):\n",
    "#         self.timing = brica.Timing(5, 1, 0)\n",
    "        self.training = training\n",
    "        self.total_steps = 0\n",
    "        self.ep_rewards = [0.]\n",
    "        self.cuda = use_cuda\n",
    "        self.ac_model = ACModel()\n",
    "        if init_weight_path is not None:\n",
    "            self.ac_model.load_state_dict(torch.load(init_weight_path))\n",
    "        self.optimizer = optim.Adam(self.ac_model.parameters(), lr=lr)\n",
    "        if self.cuda: self.ac_model = self.ac_model.cuda()\n",
    "        self.init_params()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "#         if 'from_environment' not in inputs:\n",
    "#             raise Exception('BG did not recieve from Environment')\n",
    "#         if 'from_pfc' not in inputs:\n",
    "#             raise Exception('BG did not recieve from PFC')\n",
    "#         if 'from_fef' not in inputs:\n",
    "#             raise Exception('BG did not recieve from FEF')\n",
    "        \n",
    "        obs = inputs['from_fef']\n",
    "        reward, done = inputs['from_environment']\n",
    "        dones = [done]\n",
    "        rewards = np.array([reward])\n",
    "\n",
    "        # reset the LSTM state for done envs\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32).reshape(-1))).unsqueeze(1)\n",
    "        if self.cuda: masks = masks.cuda()\n",
    "\n",
    "        self.total_steps += 1\n",
    "        self.ep_rewards = self.ep_rewards + rewards\n",
    "        # if done:\n",
    "        #     ep_rewards = 0\n",
    "        rewards = torch.from_numpy(rewards).float().unsqueeze(1)\n",
    "        if self.cuda: rewards = rewards.cuda()\n",
    "        if self.prev_actions is not None:\n",
    "            self.steps.append((\n",
    "                rewards,\n",
    "                masks,\n",
    "                self.prev_actions.clone(),\n",
    "                self.prev_policies,\n",
    "                self.prev_values.clone()\n",
    "            ))\n",
    "        obs = np.expand_dims(obs, axis=0).astype(\"float32\")\n",
    "        obs = Variable(torch.tensor(obs))\n",
    "#         print(obs.size())\n",
    "\n",
    "        # network forward pasa\n",
    "        dist_params, values, self.memory = self.ac_model(obs, self.memory)\n",
    "        mu , sigma = dist_params\n",
    "        policies = Normal(mu, sigma)\n",
    "        actions = policies.sample()\n",
    "        prob = policies.log_prob(actions)\n",
    "        self.prev_actions = actions\n",
    "        self.prev_policies = policies\n",
    "        self.prev_values = values\n",
    "\n",
    "        if self.total_steps % rollout_steps == 0 and self.training:\n",
    "            self.update()\n",
    "            self.init_params()\n",
    "        \n",
    "        if self.total_steps % save_steps == 0 and self.training:\n",
    "            cur_weights = self.ac_model.state_dict()\n",
    "#             torch.save(cur_weights, \"./data/bg.pth\")\n",
    "\n",
    "        \n",
    "        return actions.cpu().numpy().reshape(-1)\n",
    "        \n",
    "\n",
    "    def update(self):\n",
    "        self.steps.append((None, None, None, None, self.prev_values.clone()))\n",
    "        actions, values, returns, advantages, entropies = process_rollout(self.steps, self.cuda)\n",
    "        # calculate action probabilities\n",
    "        log_action_probs = self.prev_policies.log_prob(actions)\n",
    "\n",
    "        policy_loss = (-log_action_probs * Variable(advantages)).sum()\n",
    "        value_loss = (.5 * (values - Variable(returns)) ** 2.).sum()\n",
    "        entropy_loss = (-1)*entropies.sum()\n",
    "#         entropy_loss = (log_probs * probs).sum()\n",
    "\n",
    "        loss = policy_loss + value_loss * value_coeff + entropy_loss * entropy_coeff\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(self.ac_model.parameters(), grad_norm_limit)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "#         print(\"total step\", self.total_steps)\n",
    "#         print(\"Loss:\", loss.data[0])\n",
    "#         print(\"Return:\", torch.mean(returns).data[0])\n",
    "        \n",
    "\n",
    "    def init_params(self):\n",
    "        self.steps = []\n",
    "        self.memory = torch.zeros(1, memory_size)\n",
    "        if self.cuda: self.memory = self.memory.cuda()\n",
    "        self.prev_actions = None\n",
    "        self.prev_policies = None\n",
    "        self.prev_values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rollout(steps, cuda, num_workers=1):\n",
    "    # bootstrap discounted returns with final value estimates\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_workers, 1)\n",
    "    if cuda: advantages = advantages.cuda()\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "    out_actions = [None] * (len(steps) - 1)\n",
    "    out_policies = [None] * (len(steps) - 1)\n",
    "    out_values = [None] * (len(steps) - 1)\n",
    "    out_returns = [None] * (len(steps) - 1)\n",
    "    out_advantages = [None] * (len(steps) - 1)\n",
    "    out_entropies = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "        \n",
    "        out_actions[t] = actions\n",
    "        out_entropies[t] = policies.entropy()\n",
    "        out_policies[t] = policies\n",
    "        out_values[t] = values\n",
    "        out_returns[t] = returns\n",
    "        out_advantages[t] = advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    out_actions = torch.cat(out_actions, dim=0)\n",
    "    out_values = torch.cat(out_values, dim=0)\n",
    "    out_returns = torch.cat(out_returns, dim=0)\n",
    "    out_advantages = torch.cat(out_advantages, dim=0)\n",
    "    out_entropies = torch.cat(out_entropies, dim=0)\n",
    "    return (out_actions, out_values, out_returns, out_advantages, out_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/envs/wbai/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward=-53.901073329474215\n",
      "episode reward=-1176.2990973812205\n",
      "episode reward=-6400.248468657024\n",
      "episode reward=-9954.48756541229\n",
      "episode reward=-14413.643939915111\n",
      "episode reward=-17395.142354084575\n",
      "episode reward=-18547.0403373371\n",
      "episode reward=-22166.558170443564\n",
      "episode reward=-24924.99375387171\n",
      "episode reward=-23576.960319049766\n",
      "episode reward=-24038.312941941276\n",
      "episode reward=-25562.250651363975\n",
      "episode reward=-24986.590582880723\n",
      "episode reward=-23668.48060844528\n",
      "episode reward=-19141.03602959034\n",
      "episode reward=-24793.387644529215\n",
      "episode reward=-23811.635532418983\n",
      "episode reward=-12787.151739147503\n",
      "episode reward=-24155.38748681925\n",
      "episode reward=-24964.774952392898\n",
      "episode reward=-12429.21613582478\n",
      "episode reward=-18019.73949442265\n",
      "episode reward=-23708.3749800286\n",
      "episode reward=-24292.510322589176\n",
      "episode reward=-23633.06946201703\n",
      "episode reward=-22720.015925285126\n",
      "episode reward=-12423.431305826398\n",
      "episode reward=-24555.13153167581\n",
      "episode reward=-16969.55379245074\n",
      "episode reward=-22516.45847632439\n",
      "episode reward=-22515.79409509244\n",
      "episode reward=-18770.274069451378\n",
      "episode reward=-21085.111518460766\n",
      "episode reward=-22644.303234983665\n",
      "episode reward=-22117.039400259917\n",
      "episode reward=-22828.54013985355\n",
      "episode reward=-22089.69547629405\n",
      "episode reward=-14605.759032967992\n",
      "episode reward=-20832.679671265032\n",
      "episode reward=-21940.78617403161\n",
      "episode reward=-20780.855345572512\n",
      "episode reward=-15542.150401374294\n",
      "episode reward=-20945.494203918217\n",
      "episode reward=-21449.0466461018\n",
      "episode reward=-19866.32278653471\n",
      "episode reward=-24024.012208119944\n",
      "episode reward=-15348.489619563914\n",
      "episode reward=-11915.200805497218\n",
      "episode reward=-21178.169968467682\n",
      "episode reward=-20561.480451723237\n",
      "episode reward=-20710.844859850753\n",
      "episode reward=-19320.20881435234\n",
      "episode reward=-19595.97291802977\n",
      "episode reward=-19260.701134565108\n",
      "episode reward=-18990.079002645543\n",
      "episode reward=-19743.08843263606\n",
      "episode reward=-18220.241672135926\n",
      "episode reward=-18389.346241523177\n",
      "episode reward=-18205.333923204085\n",
      "episode reward=-16507.589439935542\n",
      "episode reward=-6628.008207044341\n",
      "episode reward=-17350.403913287642\n",
      "episode reward=-17297.755789923405\n",
      "episode reward=-13587.754290469982\n",
      "episode reward=-15784.44407722274\n",
      "episode reward=-16268.45805549714\n",
      "episode reward=-16478.10618908941\n",
      "episode reward=-14750.387865594574\n",
      "episode reward=-14955.217156522307\n",
      "episode reward=-14894.504880616723\n",
      "episode reward=-11762.150481530876\n",
      "episode reward=-8064.814436033065\n",
      "episode reward=-13907.586370647374\n",
      "episode reward=-14893.11412523787\n",
      "episode reward=-14534.529441392022\n",
      "episode reward=-14607.015599569784\n",
      "episode reward=-11355.293434795314\n",
      "episode reward=-13345.48819258139\n",
      "episode reward=-13546.146279595225\n",
      "episode reward=-12856.620902606694\n",
      "episode reward=-7519.659644577613\n",
      "episode reward=-10905.993108154671\n",
      "episode reward=-13448.233289169351\n",
      "episode reward=-12669.997541438275\n",
      "episode reward=-11939.814029733789\n",
      "episode reward=-10109.611173652196\n",
      "episode reward=-12334.420348251382\n",
      "episode reward=-13206.85184796635\n",
      "episode reward=-12391.096281686914\n",
      "episode reward=-11838.823008281983\n",
      "episode reward=-12105.545492247425\n",
      "episode reward=-11905.845658103864\n",
      "episode reward=-9110.740392687101\n",
      "episode reward=-10820.847175031304\n",
      "episode reward=-11426.607448918685\n",
      "episode reward=-10388.026392925341\n",
      "episode reward=-11752.465424413014\n",
      "episode reward=-11417.720667952248\n",
      "episode reward=-11357.823218235353\n",
      "episode reward=-11271.156817572442\n",
      "episode reward=-6548.003119184783\n",
      "episode reward=-10421.699512825557\n",
      "episode reward=-9925.764474805108\n",
      "episode reward=-9970.353118234836\n",
      "episode reward=-10125.01085485521\n",
      "episode reward=-7528.842600365205\n",
      "episode reward=-9908.243797523104\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "reward = 0\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_count = 0\n",
    "step = 0\n",
    "\n",
    "agent = BG()\n",
    "\n",
    "for i in range(step_size):\n",
    "    inputs = {}\n",
    "    inputs['from_fef'] = obs\n",
    "    inputs['from_environment'] = (reward, done)\n",
    "    action = agent(inputs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    episode_reward += reward\n",
    "    step += 1\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        print(\"episode reward={}\".format(episode_reward))\n",
    "\n",
    "        # Store log for tensorboard graph\n",
    "        episode_count += 1\n",
    "#         logger.log(\"episode_reward\", episode_reward, episode_count)\n",
    "            \n",
    "        episode_reward = 0\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
